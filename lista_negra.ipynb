{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuaderno de creación de lista negra de usuarios\n",
    "\n",
    "El código definido a continuación tiene como objetivo analizar las publicaciones de la carpeta [`datos`](./datos), y crear una lista de usuarios que no deberían ser considerados en el análisis de datos.\n",
    "\n",
    "Se consideraran para esta lista usuarios que sean autores de: \n",
    "\n",
    "* Publicaciones hechas para fomentar el cuidado de la salud mental\n",
    "    * Si bien se pueden encontrar palabras clave que indiquen tendencias suicidas, no necesariamente reflejan problemas mentales en quien las publica\n",
    "* Publicaciones repetidas (spam, bots, ofertas de trabajo, etc.)\n",
    "* Publicaciones de páginas de noticias\n",
    "\n",
    "Además, es posible excluir tweets que se consideren irrelevantes para el análisis, como por ejemplo:\n",
    "\n",
    "* Aquellos que usen una de las palabras claves del conjunto de datos, pero que no tengan relación con el tema\n",
    "    * Ejemplo: \"Mi hermano suele colgarme cuando le llamo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar las siguientes instrucciones, se empleará el lenguaje de R.\n",
    "\n",
    "Será de utilidad en el análisis, además, los paquetes `tm` y `SnowballC`, que permiten realizar análisis sobre texto y lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tm' was built under R version 4.2.3\"\n",
      "Loading required package: NLP\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 4.2.3\"\n",
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Warning message:\n",
      "\"package 'SnowballC' was built under R version 4.2.3\"\n"
     ]
    }
   ],
   "source": [
    "# Se cargan las librerías\n",
    "library('tm')\n",
    "library('dplyr')\n",
    "library('SnowballC')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que nada, definimos funciones que nos permitan manipular qué archivos queremos analizar.\n",
    "\n",
    "Estas funciones permiten procesar de manera genérica los archivos de la carpeta [`datos`](./datos), de tal forma que se contemplen únicamente los registros de usuarios con más de N publicaciones.\n",
    "\n",
    "Otras consideraciones sobre los datos serán más específicas y se definirán más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos una lista de archivos ya procesados, de manera que no se carguen todos en cada sesión\n",
    "archivos_procesados <- readLines('./datos/lista_procesados.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregamos además, functiones que nos permitan actualizar la lista de archivos procesados\n",
    "\n",
    "guardar_lista <- function(ruta){\n",
    "    writeLines(archivos_procesados, ruta)\n",
    "}\n",
    "\n",
    "\n",
    "actualizar_lista <- function(archivos, reemplazar = FALSE, guardar = FALSE){\n",
    "\n",
    "    if (reemplazar) {\n",
    "        archivos_procesados <- archivos\n",
    "    }\n",
    "    else {\n",
    "        archivos_procesados <- c(archivos_procesados, archivos)\n",
    "        archivos_procesados <- unique(archivos_procesados)\n",
    "    }\n",
    "\n",
    "    if (guardar) {\n",
    "        writeLines(archivos_procesados, './datos/lista_procesados.txt')\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "actualizar_lista_indicar_procesados <- function(archivos_procesando){\n",
    "\n",
    "    archivos_procesados_nuevos <- c()\n",
    "    \n",
    "    for (archivo in archivos_procesando) {\n",
    "\n",
    "        # Solicitar confirmación\n",
    "        respuesta <- readline(paste0(\"¿Desea indicar el archivo [\", archivo, \"] como procesado? (s/n)\"))\n",
    "\n",
    "        if (tolower(respuesta) == 's') {\n",
    "            archivos_procesados_nuevos <- c(archivos_procesados_nuevos, archivo)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    respuesta <- readline(\"Lista actualizada. ¿Desea guardar la lista? (s/n)\")\n",
    "\n",
    "    guardar <- FALSE\n",
    "\n",
    "    if (tolower(respuesta) == 's') {\n",
    "        guardar <- TRUE\n",
    "    }\n",
    "    \n",
    "    actualizar_lista(archivos_procesados_nuevos, reemplazar = FALSE, guardar = guardar)\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para optimizar el proceso de lectura de archivos, vamos a definir funciones que:\n",
    "\n",
    "* Descubran los archivos .csv dada una serie de carpetas base.\n",
    "* Extraigan el nombre de los mismos para renombrarlos de ser necesario.\n",
    "* Lean los archivos .csv y los conviertan en un dataframe de R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos una función que nos permita conocer las rutas de los archivos a procesar, dada una lista de carpetas base\n",
    "obtener_rutas <- function(carpetas) {\n",
    "\n",
    "    rutas <- c()\n",
    "\n",
    "    for (carpeta in carpetas) {\n",
    "\n",
    "        rutas <- c(\n",
    "            rutas, # Rutas anteriores\n",
    "            list.files(carpeta, pattern = \"*.csv\", full.names = TRUE) # Rutas de los archivos descubiertos\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return(rutas)\n",
    "}\n",
    "\n",
    "# Definimos una función que nos de el nombre del archivo a partir de su ruta\n",
    "obtener_nombre_archivo <- function(ruta) {\n",
    "\n",
    "    # Separamos la ruta por las diagonales\n",
    "    ruta_separada <- strsplit(ruta, \"/\", fixed = TRUE)[[1]]\n",
    "\n",
    "    # Obtenemos el último elemento de la ruta\n",
    "    nombre_archivo <- ruta_separada[length(ruta_separada)]\n",
    "\n",
    "    return(nombre_archivo)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, es momento de realizar una estrategia de lectura de archivos. Debido a que el volumen de tweets suministrados es grande, se optará por leer los archivos uno a uno.\n",
    "\n",
    "A continuación, únicamente relacionaremos el la ruta del archivo con su nombre, para leerlo en el momento que sea necesario y renombrar el archivo de salida de forma correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "obtener_archivos_por_leer <- function(carpetas, archivos_procesados, verbose){\n",
    "\n",
    "    # Obtenemos las rutas de los archivos a procesar\n",
    "    rutas <- obtener_rutas(carpetas)\n",
    "\n",
    "    # Se juntan las rutas con los nombres de los archivos\n",
    "\n",
    "    archivos_sin_leer <- list()\n",
    "\n",
    "    for (ruta in rutas){\n",
    "\n",
    "        nombre_archivo <- obtener_nombre_archivo(ruta)\n",
    "\n",
    "        if (!(nombre_archivo %in% archivos_procesados)) {\n",
    "            \n",
    "            # Agregamos el nombre del archivo y su ruta a la lista\n",
    "            archivos_sin_leer <- append(\n",
    "                archivos_sin_leer,\n",
    "                list( # El método append toma las dos listas y las junta en una sola\n",
    "                    list( # Por eso, la lista contiene otra lista, que es una tupla\n",
    "                        nombre = nombre_archivo,\n",
    "                        ruta = ruta\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    if (verbose) { # Si se solicita, imprimimos los archivos encontrados\n",
    "\n",
    "        if (length(archivos_sin_leer) == 0){\n",
    "            print('No se encontraron archivos nuevos.')\n",
    "        } else {\n",
    "            print('Se encontraron los siguientes archivos:')\n",
    "        }\n",
    "        \n",
    "        for (archivo in archivos_sin_leer) {\n",
    "            print(archivo$nombre)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return(archivos_sin_leer)\n",
    "}\n",
    "\n",
    "# Está función de escritura mantiene el formato en el que originalmente se encuentan los .csv con Tweets\n",
    "escribir_dataframe <- function(dataframe, ruta_archivo) {\n",
    "    write.csv(\n",
    "        dataframe,\n",
    "        file = ruta_archivo,\n",
    "        row.names = FALSE,\n",
    "        col.names = TRUE,\n",
    "        quote = TRUE\n",
    "    )\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, empleamos las funciones para obtener los archivos que leeremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Se encontraron los siguientes archivos:\"\n",
      "[1] \"necesito_ayuda_complete.csv\"\n",
      "[1] \"suicida_complete.csv\"\n"
     ]
    }
   ],
   "source": [
    "# Leeremos los archivos .csv de las carpetas especificadas\n",
    "# Hecho de esta manera, emplearemos las rutas para procesar los archivos\n",
    "carpetas_a_explorar <- c(\n",
    "    './datos/twitter/'\n",
    ")\n",
    "\n",
    "rutas_archivos <- obtener_archivos_por_leer(carpetas_a_explorar, archivos_procesados, verbose = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos los archivos que se van a procesar en una lista\n",
    "\n",
    "archivos_de_la_sesion <- c()\n",
    "\n",
    "for (archivo in rutas_archivos) {\n",
    "    archivos_de_la_sesion <- c(archivos_de_la_sesion, archivo$nombre)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, reduciremos el tamaño de los conjuntos de datos, y mantendremos sólo aquellos usuarios con N publicaciones o más.\n",
    "\n",
    "Esto nos permitirá hacer una revisión manual después, para detectar posibles usuarios que no deberían ser considerados en la construcción de los conjuntos de datos limpios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos del dataframe las filas que tengan usuarios que aparezcan más de N veces\n",
    "\n",
    "obtener_publicaciones <- function(dataframe, pub_minimas) {\n",
    "\n",
    "    dataframe <- dataframe %>% \n",
    "        group_by(User) %>% \n",
    "        filter(n() >= pub_minimas)\n",
    "\n",
    "    return(dataframe)\n",
    "}\n",
    "\n",
    "reducir_archivos <- function(archivos, pub_minimas, excepciones_archivos_a_reducir, ruta_guardado, verbose, sufijo) {\n",
    "\n",
    "    \n",
    "    for (ruta_nombre in archivos){\n",
    "\n",
    "        ruta <- ruta_nombre$ruta\n",
    "        nombre <- ruta_nombre$nombre\n",
    "\n",
    "        if (!(nombre %in% excepciones_archivos_a_reducir)){\n",
    "\n",
    "            # Leemos el archivo\n",
    "\n",
    "            if (verbose){\n",
    "                print(paste('Leyendo el archivo', nombre))\n",
    "            }\n",
    "\n",
    "            dataframe <- read.csv(file = ruta)\n",
    "\n",
    "            filas_iniciales <- nrow(dataframe)\n",
    "\n",
    "            # Obtenemos las publicaciones de los usuarios que aparezcan más de N veces\n",
    "\n",
    "            if (verbose){\n",
    "                print(paste('Filtrando usuarios con menos de', pub_minimas, 'publicaciones'))\n",
    "            }\n",
    "\n",
    "            dataframe <- obtener_publicaciones(dataframe, numero_min_publicaciones)\n",
    "\n",
    "            filas_finales <- nrow(dataframe)\n",
    "\n",
    "            # Creamos la nueva ruta. Le quitamos el .csv al nombre del archivo\n",
    "            nueva_ruta <- paste0(ruta_guardado, substr(nombre, 1, nchar(nombre) - 4), sufijo)\n",
    "\n",
    "            # Escribimos el archivo\n",
    "            escribir_dataframe(dataframe, nueva_ruta)\n",
    "\n",
    "            if (verbose){\n",
    "                print(\n",
    "                    paste('El archivo',\n",
    "                            nombre,\n",
    "                            '(# ',\n",
    "                            filas_iniciales ,\n",
    "                            'líneas) se redujo ( ahora',\n",
    "                            obtener_nombre_archivo(nueva_ruta),\n",
    "                            ', #',\n",
    "                            filas_finales,\n",
    "                            ' líneas).'\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "\n",
    "        }\n",
    "        else if (verbose){\n",
    "            print(paste('El archivo', nombre, 'no se redujo.'))\n",
    "        }\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya definimos este paso del pipeline, es momento de ejecutarlo.\n",
    "\n",
    "Toma en cuenta que necesitas crear el directorio en el que vas a guardar los archivos de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Leyendo el archivo necesito_ayuda_complete.csv\"\n",
      "[1] \"Filtrando usuarios con menos de 10 publicaciones\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"El archivo necesito_ayuda_complete.csv (#  4198034 líneas) se redujo ( ahora necesito_ayuda_complete_reducido.csv , # 748962  líneas).\"\n",
      "[1] \"Leyendo el archivo suicida_complete.csv\"\n",
      "[1] \"Filtrando usuarios con menos de 10 publicaciones\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"El archivo suicida_complete.csv (#  2816694 líneas) se redujo ( ahora suicida_complete_reducido.csv , # 1092681  líneas).\"\n"
     ]
    }
   ],
   "source": [
    "# Si lo deseamos, podemos indicar archivos que no queremos pasar por el proceso de reducción\n",
    "excepciones_archivos_a_reducir <- c(\n",
    "    \n",
    ")\n",
    "\n",
    "numero_min_publicaciones <- 10\n",
    "\n",
    "reducir_archivos(\n",
    "    rutas_archivos,\n",
    "    numero_min_publicaciones, \n",
    "    excepciones_archivos_a_reducir,\n",
    "    ruta_guardado = './datos_limpios/datos_por_revisar/twitter/',\n",
    "    verbose = TRUE,\n",
    "    sufijo = '_reducido.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Guardar el progreso en la sesión\n",
    "actualizar_lista_indicar_procesados(archivos_de_la_sesion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que existen muchas palabras de poca relevancia y múltiples frases (a veces generadas a través de aplicaciones, otras son letras de canciones que usan la palabra con otro significado, y a veces incluso son etiquetas a usuarios populares o de influencia política), las celdas siguientes atienden necesidades específicas de la limpieza de cada conjunto de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obstante, por practicidad, se declaran previamente las funciones que se emplearán para limpiar los conjuntos de datos, con el fin de declararlas en la sesión más accesiblemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Se juntan los vectores que contienen los nombres de los usuarios, para ser unificados en una sola lista negra\n",
    "# Se le suministrarán varios vectores de usuarios, y se devolverá un vector con los usuarios únicos\n",
    "\n",
    "juntar_usuarios <- function(listas_usuarios) {\n",
    "\n",
    "    usuarios <- c()\n",
    "\n",
    "    for (lista in listas_usuarios) {\n",
    "        usuarios <- c(usuarios, lista)\n",
    "    }\n",
    "\n",
    "    usuarios <- unique(usuarios)\n",
    "\n",
    "    return(usuarios)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Función que obtiene los tweets de un usuario y determina si todos contienen una serie de textos claves\n",
    "\n",
    "tweetsSonRepetitivos <- function(dataset, usuario, textosClave) {\n",
    "    # Obtenemos los tweets del usuario\n",
    "    tweets <- dataset %>%\n",
    "        filter(User == usuario) %>%\n",
    "        pull(Content) # pull() devuelve un vector con los valores de la columna\n",
    "    \n",
    "    # Revisamos si todos los tweets contienen alguno de los textos clave\n",
    "    for (texto in textosClave){\n",
    "        todosContienen <- all(grepl(texto, tweets, ignore.case = TRUE))\n",
    "\n",
    "        # Nos conformamos con saber que uno de los textos que deseamos excluir está en los tweets,\n",
    "        # pues es suficiente para determinar que el usuario repite contenido\n",
    "        if (todosContienen) {\n",
    "            return (TRUE)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return (todosContienen)\n",
    "}\n",
    "\n",
    "# Función que toma un dataset y una serie de textos clave, y devuelve los usuarios que tienen todos\n",
    "# sus tweets con al menos uno de esos textos clave\n",
    "\n",
    "obtenerUsuariosRepetitivos <- function(dataset, textosClave) {\n",
    "    # Obtenemos los usuarios\n",
    "    usuarios <- dataset %>%\n",
    "        distinct(User) %>%\n",
    "        pull(User)\n",
    "        \n",
    "    # Obtenemos los usuarios que tienen todos sus tweets con alguno de sus textos clave en este vector\n",
    "    usuarios_repetitivos <- c()\n",
    "    \n",
    "    for (usuario in usuarios) {\n",
    "\n",
    "        if (tweetsSonRepetitivos(dataset, usuario, textosClave)) {\n",
    "            usuarios_repetitivos <- c(usuarios_repetitivos, usuario)\n",
    "            break\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return (unique(usuarios_repetitivos))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "raiz_verbo_aparece <- function(texto, raices_a_comparar, lenguaje) {\n",
    "\n",
    "    palabras <- unlist(strsplit(texto, \" \"))\n",
    "    raices_palabras <- wordStem(palabras, language = lenguaje)\n",
    "    \n",
    "    for (raiz_palabra in raices_palabras) {\n",
    "        \n",
    "        for (raiz_verbo in raices_a_comparar) {\n",
    "            \n",
    "            if ( grepl(raiz_verbo, raiz_palabra, ignore.case = TRUE) ) {\n",
    "                return(TRUE)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    return(FALSE)\n",
    "\n",
    "}\n",
    "\n",
    "# Usando el paquete SnowballC, se buscarán los tweets que contengan alguna conjugación de los verbos indicados\n",
    "\n",
    "excluir_tweets_segun_verbos <- function(dataframe, palabras, language) {\n",
    "\n",
    "    # Se obtienen las raíces de las palabras\n",
    "    raices_palabras <- wordStem(palabras, language = language)\n",
    "\n",
    "    # Se obtienen los tweets que no contengan ninguna de las raíces de las palabras (i.e., el verbo no aparece\n",
    "    # en ninguna conjugación)\n",
    "    tweets_sin_palabras <- dataframe %>%\n",
    "        mutate(\n",
    "            contiene_raices_palabras = sapply(Content, raiz_verbo_aparece, raices_palabras, language)\n",
    "        ) %>%\n",
    "        filter(contiene_raices_palabras == FALSE) %>%\n",
    "        select(-contiene_raices_palabras) # Se elimina la columna auxiliar\n",
    "\n",
    "    return(tweets_sin_palabras)\n",
    "}\n",
    "\n",
    "excluir_tweets_segun_expresiones <- function(dataframe, expresiones) {\n",
    "\n",
    "    # Se obtienen los tweets que no contengan ninguna de las expresiones\n",
    "    tweets_sin_palabras <- dataframe %>%\n",
    "        mutate(\n",
    "            contiene_expresiones = as.vector(sapply(Content, comparar_tweet_con_palabras, expresiones))\n",
    "        ) %>%\n",
    "        filter(contiene_expresiones == FALSE) %>%\n",
    "        select(-contiene_expresiones) # Se elimina la columna auxiliar\n",
    "\n",
    "    return(tweets_sin_palabras)\n",
    "}\n",
    "\n",
    "comparar_tweet_con_palabras <- function(tweet, palabras) {\n",
    "\n",
    "    # Se revisa si alguna de las palabras despreciables está en el tweet\n",
    "\n",
    "    for (palabra in palabras) {\n",
    "\n",
    "        if (grepl(palabra, tweet, ignore.case = TRUE)) {\n",
    "            return(TRUE)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return(FALSE)\n",
    "\n",
    "}\n",
    "\n",
    "# Función que devuelve los usuarios que usan al menos una de las palabras despreciables indicadas en sus tweets\n",
    "\n",
    "obtener_usuarios_segun_palabras <- function(dataframe, palabras) {\n",
    "\n",
    "    # Se obtienen los usuarios que usan al menos una de las palabras despreciables indicadas en sus tweets\n",
    "    usuarios_con_palabras <- c()\n",
    "\n",
    "    # Se recorren todos los tweets, y se revisa si alguno contiene alguna de las palabras despreciables,\n",
    "    # para determinar si el usuario que lo escribió debe ser incluido en la lista negra\n",
    "\n",
    "    usuarios_con_palabras <- dataframe %>%\n",
    "        mutate(\n",
    "            contiene_palabras = as.vector(sapply(Content, comparar_tweet_con_palabras, palabras))\n",
    "        ) %>%\n",
    "        filter(contiene_palabras == TRUE) %>%\n",
    "        distinct(User)\n",
    "\n",
    "    return (usuarios_con_palabras$User)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cargar_lista_negra <- function(ruta = './datos_limpios/lista_negra_revisada.txt') {\n",
    "\n",
    "    lista_negra <- readLines(ruta)\n",
    "\n",
    "    return(lista_negra)\n",
    "}\n",
    "\n",
    "escribir_lista_negra <- function(lista_negra, ruta = './datos_limpios/lista_negra_revisada.txt', retirar_duplicados = TRUE) {\n",
    "\n",
    "    if(retirar_duplicados) {\n",
    "        lista_negra <- unique(lista_negra)\n",
    "    }\n",
    "    \n",
    "    writeLines(lista_negra, ruta)\n",
    "\n",
    "}\n",
    "\n",
    "ruta_lista_negra <- './datos_limpios/lista_negra_revisada.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "retirar_tweets_usuarios <- function(dataframe, usuarios) {\n",
    "    return (\n",
    "        dataframe %>%\n",
    "            filter(!User %in% usuarios)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets: `ahorcarme_complete.csv` y `colgarme_complete.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos los datos del archivo 'ahorcarme_complete.csv'\n",
    "publicaciones_ahorcarme <- obtener_dataframe('ahorcarme_complete.csv', datos)\n",
    "publicaciones_colgarme <- obtener_dataframe('colgarme_complete.csv', datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Filtramos los usuarios que tengan más de 5 publicaciones\n",
    "publicaciones_ahorcarme_reducidas <- obtener_publicaciones(publicaciones_ahorcarme, 5)\n",
    "publicaciones_colgarme_reducidas <- obtener_publicaciones(publicaciones_colgarme, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos la lista de los usuarios que aparecen más de 5 veces en las publicaciones\n",
    "\n",
    "usuarios_ahorcarme <- publicaciones_ahorcarme_reducidas$User\n",
    "usuarios_colgarme <- publicaciones_colgarme_reducidas$User\n",
    "\n",
    "lista_negra <- juntar_usuarios(\n",
    "    c(\n",
    "        usuarios_ahorcarme,\n",
    "        usuarios_colgarme\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'soiiviic'</li><li>'gatoacostadou'</li><li>'Lucasnun_'</li><li>'cigarettommy'</li><li>'lwtironic'</li><li>'CyanideMoon'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'soiiviic'\n",
       "\\item 'gatoacostadou'\n",
       "\\item 'Lucasnun\\_'\n",
       "\\item 'cigarettommy'\n",
       "\\item 'lwtironic'\n",
       "\\item 'CyanideMoon'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'soiiviic'\n",
       "2. 'gatoacostadou'\n",
       "3. 'Lucasnun_'\n",
       "4. 'cigarettommy'\n",
       "5. 'lwtironic'\n",
       "6. 'CyanideMoon'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"soiiviic\"      \"gatoacostadou\" \"Lucasnun_\"     \"cigarettommy\" \n",
       "[5] \"lwtironic\"     \"CyanideMoon\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(lista_negra)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, podemos revisar manualmente los tweets de los usuarios que colocamos en la lista negra bajo este criterio, para determinar si realmente sus publicaciones son despreciables para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos los tweets de estos usuarios, para revisarlos manualmente\n",
    "\n",
    "# Ahorcarme\n",
    "tweets_lisneg_ahorcarme <- publicaciones_ahorcarme_reducidas %>%\n",
    "    filter(User %in% lista_negra)\n",
    "\n",
    "# Colgarme\n",
    "tweets_lisneg_colgarme <- publicaciones_colgarme_reducidas %>%\n",
    "    filter(User %in% lista_negra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos los tweets en un archivo\n",
    "\n",
    "# Ahorcarme\n",
    "escribir_dataframe(tweets_lisneg_ahorcarme, \"./datos_limpios/datos_por_revisar/tweets_lisneg_ahorcarme.csv\")\n",
    "# Colgarme\n",
    "escribir_dataframe(tweets_lisneg_colgarme, \"./datos_limpios/datos_por_revisar/tweets_lisneg_colgarme.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de revisar los tweets, encontré un fragmento que hace alusión a una connotación sexual de la palabra concurrente. Por lo tanto, aquellos usuarios cuyas publicaciones que contienen la palabra \"ahorcarme\" serán añadidos a la lista negra.\n",
    "\n",
    "Es importante considerar que, si bien emplean la palabra bajo un concepto sexual, puede no ser la única forma en la que la usan; de hecho, logré encontrar varios perfiles en los que hacen un uso sexual y violento del verbo. De tal manera, de este grupo de usuarios que tergiversan de esta forma la acción, únicamente se seleccionarán aquellos que emplean la palabra exclusivamente en un contexto sexual, es decir, en todos los tweets que les fueron recolectados que les pertenecen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fragmento\n",
    "frags_frase = c(\n",
    "    \"ahorcame por dios ahorcame hasta el punto de que tus manos\"\n",
    ")\n",
    "\n",
    "# Localizar los tweets que contengan la frase, y de ellos tomar los usuarios, para agregarlos a los que ya\n",
    "# estaban listados, siempre y cuando todos los tweets del usuario contengan la frase\n",
    "\n",
    "usuarios_frags_ahorcarme <- obtenerUsuariosRepetitivos(publicaciones_ahorcarme, frags_frase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro son fragmentos de una canción, en los tweets con la palabra clave \"colgarme\"\n",
    "\n",
    "Los fragmentos son los siguientes:\n",
    "\n",
    "<blockquote>\n",
    "    <ul>\n",
    "        <li> Niña, dame una pestaña de tus ojos para colgarme de amor por ti </li>\n",
    "        <li> Colgarme de cualquiera que le gusta trasnochar </li>\n",
    "    </ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fragmentos\n",
    "frags_frase = c(\n",
    "    \"Niña, dame una pestaña de tus ojos para colgarme de amor por ti\",\n",
    "    \"Colgarme de cualquiera que le gusta trasnochar\"\n",
    ")\n",
    "# Localizar los tweets que contengan la frase, y de ellos tomar los usuarios, para agregarlos a los que ya\n",
    "# estaban listados.\n",
    "\n",
    "usuarios_frags_colgarme <- obtenerUsuariosRepetitivos(publicaciones_colgarme, frags_frase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igualmente, se removerá cualquier uso de la llamada en el contexto de una conversación telefónica.\n",
    "\n",
    "Por su parte, otros usos de la palabra que se refieran a una acción como \"depender de\" o \"aprovecharse de\" podrían requerir de otro tipo de servicios para ser procesados.\n",
    "\n",
    "En [este notebook](./filtrado_publicaciones_por_contexto.ipynb) estaré trabajando en otros usos de la palabra a través de los servicios que ofrece Microsoft Azure. Indagaré en el uso de los servicios cognitivos o las integraciones con OpenAI para proveer un filtro de las publicaciones con un procesamiento del lenguaje más intuitivo y preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"llam\"    \"marc\"    \"respond\" \"contest\"\n"
     ]
    }
   ],
   "source": [
    "# No quitamos al usuario completamente porque el uso de la palabra \"colgar\" en una llamada no le excluye de usarla con\n",
    "# intenciones autolesivas, por lo que nos limitamos a excluir esas publicaciones\n",
    "\n",
    "palabras_contexto_telefonico <- c(\n",
    "    \"llamar\",\n",
    "    \"marcar\",\n",
    "    \"responder\",\n",
    "    \"contestar\"\n",
    ")\n",
    "\n",
    "# Demostrando el uso del paquete tm\n",
    "raices_palabras_contexto_telefonico <- wordStem(palabras_contexto_telefonico, language = \"spanish\")\n",
    "cat(raices_palabras_contexto_telefonico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminamos los tweets que contengan alguna conjugación de los verbos que indican que se habla de una llamada\n",
    "\n",
    "tweets_colgarme_limpios <- excluir_tweets_segun_verbos(\n",
    "    publicaciones_colgarme,\n",
    "    palabras_contexto_telefonico,\n",
    "    \"spanish\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Escribir tweets para no depender de la sesión de R\n",
    "escribir_dataframe(tweets_colgarme_limpios, \"./datos_limpios/datos_por_revisar/tweets_colgarme_limpios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Escribir los mismos tweets, pero extrayendo aquellos cuyos usuarios aparecen con frecuencia\n",
    "\n",
    "escribir_dataframe(\n",
    "    obtener_publicaciones(tweets_colgarme_limpios, 5),\n",
    "    \"./datos_limpios/datos_por_revisar/tweets_colgarme_limpios_5_ocurrencias.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregar los usuarios a la lista de usuarios\n",
    "\n",
    "lista_negra <- juntar_usuarios(\n",
    "    c(\n",
    "        lista_negra,\n",
    "        usuarios_frags_ahorcarme,\n",
    "        usuarios_frags_colgarme\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Escribimos las listas de usuarios para no depender de la sesión de R\n",
    "writeLines(usuarios_frags_ahorcarme, \"./datos_limpios/datos_por_revisar/usuarios_frag_ahorcarme.txt\")\n",
    "writeLines(usuarios_frags_colgarme, \"./datos_limpios/datos_por_revisar/usuarios_frag_colgarme.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Escribir la lista negra en un archivo\n",
    "writeLines(\n",
    "    lista_negra,\n",
    "    file = \"./datos_limpios/datos_por_revisar/lista_negra.txt\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de aquí, una vez que hallamos revisado la lista negra de usuarios, vamos a limpiar los conjuntos de datos para obtener únicamente la información útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Excluir a estos usuarios de los datasets, pero a partir de la lista negra de usuarios ya revisada\n",
    "\n",
    "# Leer lista\n",
    "lista_negra_revisada <- readLines(\"./datos_limpios/lista_negra_revisada.txt\", encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Reescribir lista negra, para obtener sólo valores únicos\n",
    "lista_negra_revisada <- unique(lista_negra_revisada) %>% sort()\n",
    "writeLines(lista_negra_revisada, \"./datos_limpios/lista_negra_revisada.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ahorcarme\n",
    "publicaciones_ahorcarme <- retirar_tweets_usuarios(publicaciones_ahorcarme, lista_negra_revisada)\n",
    "\n",
    "# Colgarme\n",
    "publicaciones_colgarme <- retirar_tweets_usuarios(tweets_colgarme_limpios, lista_negra_revisada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos los tweets en un archivo\n",
    "\n",
    "# Ahorcarme\n",
    "escribir_dataframe(publicaciones_ahorcarme, \"./datos_limpios/ahorcarme_filtered.csv\")\n",
    "\n",
    "# Colgarme\n",
    "escribir_dataframe(publicaciones_colgarme, \"./datos_limpios/colgarme_filtered.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset `necesito_ayuda_complete.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset encontramos las siguientes stopwords:\n",
    "\n",
    "- Estoy jugando #VenezuelaQuiz y necesito ayuda con esta imagen ¿La reconoces?\n",
    "- ¡Necesito ayuda con mis árboles!\n",
    "- Entra aquí\n",
    "- Entra en\n",
    "- _Usuarios con “1D” en su nombre (fans de One Direction solicitando ayuda para conseguir boletos)_\n",
    "- One Direction\n",
    "- sorteo\n",
    "- concurso\n",
    "- Sigan a\n",
    "- Síganme\n",
    "- rt\n",
    "- retweet\n",
    "- MG\n",
    "- MG AL COMENTARIO\n",
    "- bonus\n",
    "\n",
    "Las usaremos para obtener a usuarios que utilicen esas frases en sus publicaciones, y los añadiremos a la lista negra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Se lee el dataset\n",
    "\n",
    "datos_necesito_ayuda <- read.csv(file = \"./datos/twitter/necesito_ayuda_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_necesito_ayuda <- c(\n",
    "    \"Estoy jugando #VenezuelaQuiz y necesito ayuda con esta imagen ¿La reconoces?\",\n",
    "    \"¡Necesito ayuda con mis árboles!\",\n",
    "    \"Entra aquí\",\n",
    "    \"Entra en\",\n",
    "    \"1D\",\n",
    "    \"One Direction\",\n",
    "    \"sorteo\",\n",
    "    \"concurso\",\n",
    "    \"Sigan a\",\n",
    "    \"Siganme\",\n",
    "    \"Síganme\",\n",
    "    \"rt\",\n",
    "    \"retweet\",\n",
    "    \"retuit\",\n",
    "    \"MG\",\n",
    "    \"MG AL COMENTARIO\",\n",
    "    \"bonus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_negra_necesito_ayuda <- obtener_usuarios_segun_palabras(\n",
    "    dataframe = datos_necesito_ayuda,\n",
    "    palabras = stopwords_necesito_ayuda\n",
    ")\n",
    "\n",
    "datos_necesito_ayuda_limpios <- retirar_tweets_usuarios(\n",
    "    datos_necesito_ayuda,\n",
    "    usuarios_ls_negra_necesito_ayuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in readLines(ruta):\n",
      "\"incomplete final line found on './datos_limpios/lista_negra_revisada.txt'\"\n"
     ]
    }
   ],
   "source": [
    "# Agregamos los usuarios a la lista negra\n",
    "lista_negra_revisada <- cargar_lista_negra()\n",
    "lista_negra_revisada <- juntar_usuarios(\n",
    "    c(\n",
    "        lista_negra_revisada,\n",
    "        usuarios_ls_negra_necesito_ayuda\n",
    "    )\n",
    ")\n",
    "\n",
    "escribir_lista_negra(lista_negra_revisada)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, retiraremos tweets que contengan enlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "datos_necesito_ayuda_limpios <- excluir_tweets_segun_expresiones(\n",
    "    dataframe = datos_necesito_ayuda_limpios,\n",
    "    expresiones = c(\n",
    "        \"https://\",\n",
    "        \"http://\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    }
   ],
   "source": [
    "escribir_dataframe(\n",
    "    datos_necesito_ayuda_limpios,\n",
    "    \"./datos_limpios/twitter/necesito_ayuda_cleaned.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos, además, propaganda política en los tweets de los usuarios. A menudo, etiquetan al (al año de 2023) presidente de Venezuela, Nicolás Maduro, en sus publicaciones.\n",
    "\n",
    "No obstante, si revisamos estas cuentas podremos percatarnos de que en sus publicaciones existen peticiones por recursos, pues son personas en situación de calle, desempleo, o que no tienen acceso a servicios básicos. Por ello, se tomarán estos usuarios y se colocaran en una \"lista gris\", para que puedan ser examinados después por especialistas y determinar si su situación les puede suscitar conductas depresivas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset `perdon_por_todo_complete.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset encontramos estas frases que podrían ser de interés excluir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "expresiones_perdon_por_todo <- c(\n",
    "    \"Perdón por ser fiel, entregar todo de mí, quererte como a nadie\",\n",
    "    \"Gracias por enseñarme lo que debo mejorar\",\n",
    "    \"al querer ser perfecto se equivoca todo el mundo\",\n",
    "    \"Ya no queda valor para mirarnos de nuevo y pedirnos\",\n",
    "    \"Gracias por todo y perdón por ser tan básico\",\n",
    "    \"Corazón, perdón por todo el daño\"\n",
    ")\n",
    "\n",
    "datos_perdon_por_todo <- read.csv(file = \"./datos/twitter/perdon_por_todo_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_negra_perdon <- obtenerUsuariosRepetitivos(\n",
    "    dataset = datos_perdon_por_todo,\n",
    "    textosClave = expresiones_perdon_por_todo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregamos los usuarios a la lista negra\n",
    "lista_negra_revisada <- cargar_lista_negra()\n",
    "lista_negra_revisada <- juntar_usuarios(\n",
    "    c(\n",
    "        lista_negra_revisada,\n",
    "        usuarios_ls_negra_necesito_ayuda\n",
    "    )\n",
    ")\n",
    "\n",
    "escribir_lista_negra(lista_negra_revisada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Retiramos los usuarios del dataset y los guardamos en un archivo\n",
    "datos_perdon_por_todo_limpios <- retirar_tweets_usuarios(\n",
    "    datos_perdon_por_todo,\n",
    "    usuarios_ls_negra_perdon\n",
    ")\n",
    "\n",
    "# Retiramos los tweets con URLs\n",
    "\n",
    "datos_perdon_por_todo_limpios <- excluir_tweets_segun_expresiones(\n",
    "    dataframe = datos_perdon_por_todo_limpios,\n",
    "    expresiones = c(\n",
    "        \"https://\",\n",
    "        \"http://\",\n",
    "    )\n",
    ")\n",
    "\n",
    "escribir_dataframe(\n",
    "    datos_perdon_por_todo_limpios,\n",
    "    \"./datos_limpios/perdon_por_todo_cleaned.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
