{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuaderno de creación de lista negra de usuarios\n",
    "\n",
    "El código definido a continuación tiene como objetivo analizar las publicaciones de la carpeta [`datos`](./datos), y crear una lista de usuarios que no deberían ser considerados en el análisis de datos.\n",
    "\n",
    "Se consideraran para esta lista usuarios que sean autores de: \n",
    "\n",
    "* Publicaciones hechas para fomentar el cuidado de la salud mental\n",
    "    * Si bien se pueden encontrar palabras clave que indiquen tendencias suicidas, no necesariamente reflejan problemas mentales en quien las publica\n",
    "* Publicaciones repetidas (spam, bots, ofertas de trabajo, etc.)\n",
    "* Publicaciones de páginas de noticias\n",
    "\n",
    "Además, es posible excluir tweets que se consideren irrelevantes para el análisis, como por ejemplo:\n",
    "\n",
    "* Aquellos que usen una de las palabras claves del conjunto de datos, pero que no tengan relación con el tema\n",
    "    * Ejemplo: \"Mi hermano suele colgarme cuando le llamo\"\n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "### Sobre los datos\n",
    "\n",
    "Los datos procesados aquí provienen, en su mayoría, de Twitter, y fueron obtenidos usando una serie de palabras/frases clave por parte del equipo.\n",
    "\n",
    "Además, se procesará un conjunto de textos clasificados como \"con riesgo suicida\" y \"sin riesgo suicida\". Estos textos se obtuvieron de un repositorio en línea por el director del proyecto, y no se procesan en un notebook adicional debido a que aquí ya se encuentra la funcionalidad necesaria para procesarlos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar las siguientes instrucciones, se empleará el lenguaje de R.\n",
    "\n",
    "Será de utilidad en el análisis, además, los paquetes `tm` y `SnowballC`, que permiten realizar análisis sobre texto y lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tm' was built under R version 4.2.3\"\n",
      "Loading required package: NLP\n",
      "\n",
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 4.2.3\"\n",
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Warning message:\n",
      "\"package 'SnowballC' was built under R version 4.2.3\"\n",
      "Warning message:\n",
      "\"package 'stringr' was built under R version 4.2.3\"\n"
     ]
    }
   ],
   "source": [
    "# Se cargan las librerías\n",
    "library('tm')\n",
    "library('dplyr')\n",
    "library('SnowballC')\n",
    "library('stringr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que nada, definimos funciones que nos permitan manipular qué archivos queremos analizar.\n",
    "\n",
    "Estas funciones permiten procesar de manera genérica los archivos de la carpeta [`datos`](./datos), de tal forma que se contemplen únicamente los registros de usuarios con más de N publicaciones.\n",
    "\n",
    "Otras consideraciones sobre los datos serán más específicas y se definirán más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos una lista de archivos ya procesados, de manera que no se carguen todos en cada sesión\n",
    "archivos_procesados <- readLines('./datos/lista_procesados.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregamos además, functiones que nos permitan actualizar la lista de archivos procesados\n",
    "\n",
    "guardar_lista <- function(ruta){\n",
    "    writeLines(archivos_procesados, ruta)\n",
    "}\n",
    "\n",
    "\n",
    "actualizar_lista <- function(archivos, reemplazar = FALSE, guardar = FALSE){\n",
    "\n",
    "    if (reemplazar) {\n",
    "        archivos_procesados <- archivos\n",
    "    }\n",
    "    else {\n",
    "        archivos_procesados <- c(archivos_procesados, archivos)\n",
    "        archivos_procesados <- unique(archivos_procesados)\n",
    "    }\n",
    "\n",
    "    if (guardar) {\n",
    "        writeLines(archivos_procesados, './datos/lista_procesados.txt')\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "actualizar_lista_indicar_procesados <- function(archivos_procesando){\n",
    "\n",
    "    archivos_procesados_nuevos <- c()\n",
    "    \n",
    "    for (archivo in archivos_procesando) {\n",
    "\n",
    "        # Solicitar confirmación\n",
    "        respuesta <- readline(paste0(\"¿Desea indicar el archivo [\", archivo, \"] como procesado? (s/n)\"))\n",
    "\n",
    "        if (tolower(respuesta) == 's') {\n",
    "            archivos_procesados_nuevos <- c(archivos_procesados_nuevos, archivo)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    respuesta <- readline(\"Lista actualizada. ¿Desea guardar la lista? (s/n)\")\n",
    "\n",
    "    guardar <- FALSE\n",
    "\n",
    "    if (tolower(respuesta) == 's') {\n",
    "        guardar <- TRUE\n",
    "    }\n",
    "    \n",
    "    actualizar_lista(archivos_procesados_nuevos, reemplazar = FALSE, guardar = guardar)\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para optimizar el proceso de lectura de archivos, vamos a definir funciones que:\n",
    "\n",
    "* Descubran los archivos .csv dada una serie de carpetas base.\n",
    "* Extraigan el nombre de los mismos para renombrarlos de ser necesario.\n",
    "* Lean los archivos .csv y los conviertan en un dataframe de R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos una función que nos permita conocer las rutas de los archivos a procesar, dada una lista de carpetas base\n",
    "obtener_rutas <- function(carpetas) {\n",
    "\n",
    "    rutas <- c()\n",
    "\n",
    "    for (carpeta in carpetas) {\n",
    "\n",
    "        rutas <- c(\n",
    "            rutas, # Rutas anteriores\n",
    "            list.files(carpeta, pattern = \"*.csv\", full.names = TRUE) # Rutas de los archivos descubiertos\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return(rutas)\n",
    "}\n",
    "\n",
    "# Definimos una función que nos de el nombre del archivo a partir de su ruta\n",
    "obtener_nombre_archivo <- function(ruta) {\n",
    "\n",
    "    # Separamos la ruta por las diagonales\n",
    "    ruta_separada <- strsplit(ruta, \"/\", fixed = TRUE)[[1]]\n",
    "\n",
    "    # Obtenemos el último elemento de la ruta\n",
    "    nombre_archivo <- ruta_separada[length(ruta_separada)]\n",
    "\n",
    "    return(nombre_archivo)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, es momento de realizar una estrategia de lectura de archivos. Debido a que el volumen de tweets suministrados es grande, se optará por leer los archivos uno a uno.\n",
    "\n",
    "A continuación, únicamente relacionaremos el la ruta del archivo con su nombre, para leerlo en el momento que sea necesario y renombrar el archivo de salida de forma correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "obtener_archivos_por_leer <- function(carpetas, archivos_procesados, verbose){\n",
    "\n",
    "    # Obtenemos las rutas de los archivos a procesar\n",
    "    rutas <- obtener_rutas(carpetas)\n",
    "\n",
    "    # Se juntan las rutas con los nombres de los archivos\n",
    "\n",
    "    archivos_sin_leer <- list()\n",
    "\n",
    "    for (ruta in rutas){\n",
    "\n",
    "        nombre_archivo <- obtener_nombre_archivo(ruta)\n",
    "\n",
    "        if (!(nombre_archivo %in% archivos_procesados)) {\n",
    "            \n",
    "            # Agregamos el nombre del archivo y su ruta a la lista\n",
    "            archivos_sin_leer <- append(\n",
    "                archivos_sin_leer,\n",
    "                list( # El método append toma las dos listas y las junta en una sola\n",
    "                    list( # Por eso, la lista contiene otra lista, que es una tupla\n",
    "                        nombre = nombre_archivo,\n",
    "                        ruta = ruta\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    if (verbose) { # Si se solicita, imprimimos los archivos encontrados\n",
    "\n",
    "        if (length(archivos_sin_leer) == 0){\n",
    "            print('No se encontraron archivos nuevos.')\n",
    "        } else {\n",
    "            print('Se encontraron los siguientes archivos:')\n",
    "        }\n",
    "        \n",
    "        for (archivo in archivos_sin_leer) {\n",
    "            print(archivo$nombre)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return(archivos_sin_leer)\n",
    "}\n",
    "\n",
    "# Está función de escritura mantiene el formato en el que originalmente se encuentan los .csv con Tweets\n",
    "escribir_dataframe <- function(dataframe, ruta_archivo) {\n",
    "    write.csv(\n",
    "        dataframe,\n",
    "        file = ruta_archivo,\n",
    "        row.names = FALSE,\n",
    "        col.names = TRUE,\n",
    "        quote = TRUE\n",
    "    )\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, empleamos las funciones para obtener los archivos que leeremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Se encontraron los siguientes archivos:\"\n",
      "[1] \"matarme_complete.csv\"\n"
     ]
    }
   ],
   "source": [
    "# Leeremos los archivos .csv de las carpetas especificadas\n",
    "# Hecho de esta manera, emplearemos las rutas para procesar los archivos\n",
    "carpetas_a_explorar <- c(\n",
    "    './datos/twitter/'\n",
    ")\n",
    "\n",
    "rutas_archivos <- obtener_archivos_por_leer(carpetas_a_explorar, archivos_procesados, verbose = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos los archivos que se van a procesar en una lista\n",
    "\n",
    "archivos_de_la_sesion <- c()\n",
    "\n",
    "for (archivo in rutas_archivos) {\n",
    "    archivos_de_la_sesion <- c(archivos_de_la_sesion, archivo$nombre)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después, reduciremos el tamaño de los conjuntos de datos, y mantendremos sólo aquellos usuarios con N publicaciones o más.\n",
    "\n",
    "Esto nos permitirá hacer una revisión manual después, para detectar posibles usuarios que no deberían ser considerados en la construcción de los conjuntos de datos limpios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos del dataframe las filas que tengan usuarios que aparezcan más de N veces\n",
    "\n",
    "obtener_publicaciones <- function(dataframe, pub_minimas) {\n",
    "\n",
    "    dataframe <- dataframe %>% \n",
    "        group_by(User) %>% \n",
    "        filter(n() >= pub_minimas)\n",
    "\n",
    "    return(dataframe)\n",
    "}\n",
    "\n",
    "reducir_archivos <- function(archivos, pub_minimas, excepciones_archivos_a_reducir, ruta_guardado, verbose, sufijo) {\n",
    "\n",
    "    \n",
    "    for (ruta_nombre in archivos){\n",
    "\n",
    "        ruta <- ruta_nombre$ruta\n",
    "        nombre <- ruta_nombre$nombre\n",
    "\n",
    "        if (!(nombre %in% excepciones_archivos_a_reducir)){\n",
    "\n",
    "            # Leemos el archivo\n",
    "\n",
    "            if (verbose){\n",
    "                print(paste('Leyendo el archivo', nombre))\n",
    "            }\n",
    "\n",
    "            dataframe <- read.csv(file = ruta)\n",
    "\n",
    "            filas_iniciales <- nrow(dataframe)\n",
    "\n",
    "            # Obtenemos las publicaciones de los usuarios que aparezcan más de N veces\n",
    "\n",
    "            if (verbose){\n",
    "                print(paste('Filtrando usuarios con menos de', pub_minimas, 'publicaciones'))\n",
    "            }\n",
    "\n",
    "            dataframe <- obtener_publicaciones(dataframe, numero_min_publicaciones)\n",
    "\n",
    "            filas_finales <- nrow(dataframe)\n",
    "\n",
    "            # Creamos la nueva ruta. Le quitamos el .csv al nombre del archivo\n",
    "            nueva_ruta <- paste0(ruta_guardado, substr(nombre, 1, nchar(nombre) - 4), sufijo)\n",
    "\n",
    "            # Escribimos el archivo\n",
    "            escribir_dataframe(dataframe, nueva_ruta)\n",
    "\n",
    "            if (verbose){\n",
    "                print(\n",
    "                    paste('El archivo',\n",
    "                            nombre,\n",
    "                            '(# ',\n",
    "                            filas_iniciales ,\n",
    "                            'líneas) se redujo ( ahora',\n",
    "                            obtener_nombre_archivo(nueva_ruta),\n",
    "                            ', #',\n",
    "                            filas_finales,\n",
    "                            ' líneas).'\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "\n",
    "        }\n",
    "        else if (verbose){\n",
    "            print(paste('El archivo', nombre, 'no se redujo.'))\n",
    "        }\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya definimos este paso del pipeline, es momento de ejecutarlo.\n",
    "\n",
    "Toma en cuenta que necesitas crear el directorio en el que vas a guardar los archivos de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Leyendo el archivo matarme_complete.csv\"\n",
      "[1] \"Filtrando usuarios con menos de 10 publicaciones\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"El archivo matarme_complete.csv (#  3960745 líneas) se redujo ( ahora matarme_complete_reducido.csv , # 770485  líneas).\"\n"
     ]
    }
   ],
   "source": [
    "# Si lo deseamos, podemos indicar archivos que no queremos pasar por el proceso de reducción\n",
    "excepciones_archivos_a_reducir <- c(\n",
    "    \n",
    ")\n",
    "\n",
    "numero_min_publicaciones <- 10\n",
    "\n",
    "reducir_archivos(\n",
    "    rutas_archivos,\n",
    "    numero_min_publicaciones, \n",
    "    excepciones_archivos_a_reducir,\n",
    "    ruta_guardado = './datos_limpios/datos_por_revisar/twitter/',\n",
    "    verbose = TRUE,\n",
    "    sufijo = '_reducido.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Guardar el progreso en la sesión\n",
    "actualizar_lista_indicar_procesados(archivos_de_la_sesion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que existen muchas palabras de poca relevancia y múltiples frases (a veces generadas a través de aplicaciones, otras son letras de canciones que usan la palabra con otro significado, y a veces incluso son etiquetas a usuarios populares o de influencia política), las celdas siguientes atienden necesidades específicas de la limpieza de cada conjunto de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obstante, por practicidad, se declaran previamente las funciones que se emplearán para limpiar los conjuntos de datos, con el fin de declararlas en la sesión más accesiblemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Se juntan los vectores que contienen los nombres de los usuarios, para ser unificados en una sola lista negra\n",
    "# Se le suministrarán varios vectores de usuarios, y se devolverá un vector con los usuarios únicos\n",
    "\n",
    "juntar_usuarios <- function(listas_usuarios, ordenar = FALSE) {\n",
    "\n",
    "    usuarios <- c()\n",
    "\n",
    "    for (lista in listas_usuarios) {\n",
    "        usuarios <- c(usuarios, lista)\n",
    "    }\n",
    "\n",
    "    usuarios <- unique(usuarios)\n",
    "\n",
    "    if (ordenar) {\n",
    "        usuarios <- sort(usuarios)\n",
    "    }\n",
    "\n",
    "    return(usuarios)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Función que obtiene los tweets de un usuario y determina si todos contienen un texto clave\n",
    "# de una lista dada\n",
    "\n",
    "tweetsSonRepetitivos <- function(dataset, usuario, textosClave, ignorarSignosPunt = FALSE) {\n",
    "    # Obtenemos los tweets del usuario\n",
    "    tweets <- dataset %>%\n",
    "        filter(User == usuario) %>%\n",
    "        pull(Content) # pull() devuelve un vector con los valores de la columna\n",
    "    \n",
    "    # Revisamos si todos los tweets contienen alguno de los textos clave\n",
    "    for (texto in textosClave){\n",
    "        todosContienen <- all(grepl(texto, tweets, ignore.case = TRUE))\n",
    "\n",
    "        # Nos conformamos con saber que uno de los textos que deseamos excluir está en los tweets,\n",
    "        # pues es suficiente para determinar que el usuario repite contenido\n",
    "        if (todosContienen) {\n",
    "            return (TRUE)\n",
    "        }\n",
    "\n",
    "        if(ignorarSignosPunt) {\n",
    "            # Si no todos los tweets contienen el texto clave, pero ignoramos los signos de puntuación,\n",
    "            # entonces volvemos a revisar\n",
    "\n",
    "            texto_sin_signos <- gsub(\"[[:punct:]]\", \"\", texto)\n",
    "\n",
    "            todosContienen <- all(grepl(texto_sin_signos, tweets, ignore.case = TRUE))\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    return (todosContienen)\n",
    "}\n",
    "\n",
    "# Función que toma un dataset y una serie de textos clave, y devuelve los usuarios que tienen todos\n",
    "# sus tweets con al menos uno de esos textos clave\n",
    "\n",
    "obtenerUsuariosRepetitivos <- function(dataset, textosClave, ignorarSignosPunt = FALSE) {\n",
    "    # Obtenemos los usuarios\n",
    "    usuarios <- dataset %>%\n",
    "        distinct(User) %>%\n",
    "        pull(User)\n",
    "    \n",
    "    # Obtenemos los usuarios que tienen todos sus tweets con alguno de sus textos clave en este vector\n",
    "    usuarios_repetitivos <- c()\n",
    "    \n",
    "    for (usuario in usuarios) {\n",
    "\n",
    "        if (tweetsSonRepetitivos(dataset, usuario, textosClave, ignorarSignosPunt)) {\n",
    "            usuarios_repetitivos <- c(usuarios_repetitivos, usuario)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return (usuarios_repetitivos)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "raiz_verbo_aparece <- function(texto, raices_a_comparar, lenguaje) {\n",
    "\n",
    "    palabras <- unlist(strsplit(texto, \" \"))\n",
    "    raices_palabras <- wordStem(palabras, language = lenguaje)\n",
    "    \n",
    "    for (raiz_palabra in raices_palabras) {\n",
    "        \n",
    "        for (raiz_verbo in raices_a_comparar) {\n",
    "            \n",
    "            if ( grepl(raiz_verbo, raiz_palabra, ignore.case = TRUE) ) {\n",
    "                return(TRUE)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    return(FALSE)\n",
    "\n",
    "}\n",
    "\n",
    "# Usando el paquete SnowballC, se buscarán los tweets que contengan alguna conjugación de los verbos indicados\n",
    "\n",
    "excluir_tweets_segun_verbos <- function(dataframe, palabras, language) {\n",
    "\n",
    "    # Se obtienen las raíces de las palabras\n",
    "    raices_palabras <- wordStem(palabras, language = language)\n",
    "\n",
    "    # Se obtienen los tweets que no contengan ninguna de las raíces de las palabras (i.e., el verbo no aparece\n",
    "    # en ninguna conjugación)\n",
    "    tweets_sin_palabras <- dataframe %>%\n",
    "        mutate(\n",
    "            contiene_raices_palabras = sapply(Content, raiz_verbo_aparece, raices_palabras, language)\n",
    "        ) %>%\n",
    "        filter(contiene_raices_palabras == FALSE) %>%\n",
    "        select(-contiene_raices_palabras) # Se elimina la columna auxiliar\n",
    "\n",
    "    return(tweets_sin_palabras)\n",
    "}\n",
    "\n",
    "excluir_tweets_segun_expresiones <- function(dataframe, expresiones) {\n",
    "\n",
    "    # Se obtienen los tweets que no contengan ninguna de las expresiones\n",
    "    tweets_sin_palabras <- dataframe %>%\n",
    "        mutate(\n",
    "            contiene_expresiones = as.vector(sapply(Content, comparar_tweet_con_palabras, expresiones))\n",
    "        ) %>%\n",
    "        filter(contiene_expresiones == FALSE) %>%\n",
    "        select(-contiene_expresiones) # Se elimina la columna auxiliar\n",
    "\n",
    "    return(tweets_sin_palabras)\n",
    "}\n",
    "\n",
    "comparar_tweet_con_palabras <- function(tweet, palabras) {\n",
    "\n",
    "    # Se revisa si alguna de las palabras despreciables está en el tweet\n",
    "\n",
    "    for (palabra in palabras) {\n",
    "\n",
    "        if (grepl(palabra, tweet, ignore.case = TRUE)) {\n",
    "            return(TRUE)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return(FALSE)\n",
    "\n",
    "}\n",
    "\n",
    "# Función que devuelve los usuarios que usan al menos una de las palabras despreciables indicadas en sus tweets\n",
    "\n",
    "obtener_usuarios_segun_palabras <- function(dataframe, palabras) {\n",
    "\n",
    "    # Se obtienen los usuarios que usan al menos una de las palabras despreciables indicadas en sus tweets\n",
    "    usuarios_con_palabras <- c()\n",
    "\n",
    "    # Se recorren todos los tweets, y se revisa si alguno contiene alguna de las palabras despreciables,\n",
    "    # para determinar si el usuario que lo escribió debe ser incluido en la lista negra\n",
    "\n",
    "    usuarios_con_palabras <- dataframe %>%\n",
    "        mutate(\n",
    "            contiene_palabras = as.vector(sapply(Content, comparar_tweet_con_palabras, palabras))\n",
    "        ) %>%\n",
    "        filter(contiene_palabras == TRUE) %>%\n",
    "        distinct(User) %>%\n",
    "        pull(User)\n",
    "\n",
    "    return (usuarios_con_palabras)\n",
    "\n",
    "}\n",
    "\n",
    "retirar_tweets_con_enlaces <- function(dataframe) {\n",
    "\n",
    "    formatos <- c(\n",
    "        'http://',\n",
    "        'https://',\n",
    "        'www.'\n",
    "    )\n",
    "\n",
    "    # Se obtienen los tweets que no contengan ninguna de las expresiones\n",
    "    tweets_sin_enlaces <- excluir_tweets_segun_expresiones(dataframe, formatos)\n",
    "\n",
    "    return(tweets_sin_enlaces)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ruta_lista_negra <- './datos_limpios/lista_negra_revisada.txt'\n",
    "\n",
    "cargar_lista_negra <- function(ruta = ruta_lista_negra) {\n",
    "\n",
    "    lista_negra <- readLines(ruta)\n",
    "\n",
    "    return(lista_negra)\n",
    "}\n",
    "\n",
    "escribir_lista_negra <- function(lista_negra, ruta = ruta_lista_negra, retirar_duplicados = TRUE) {\n",
    "\n",
    "    if(retirar_duplicados) {\n",
    "        lista_negra <- unique(lista_negra)\n",
    "    }\n",
    "    \n",
    "    writeLines(lista_negra, ruta)\n",
    "\n",
    "}\n",
    "\n",
    "anexar_a_lista <- function(usuarios, ruta = ruta_lista_negra) {\n",
    "\n",
    "    # Por defecto anexa los usuarios a la lista negra\n",
    "\n",
    "    conexion <- file(ruta, open = 'a')\n",
    "    writeLines(usuarios, conexion)\n",
    "    close(conexion)\n",
    "\n",
    "}\n",
    "\n",
    "ordenar_lista_negra <- function(ruta = ruta_lista_negra) {\n",
    "\n",
    "    lista_negra <- cargar_lista_negra(ruta)\n",
    "\n",
    "    lista_negra <- sort(lista_negra)\n",
    "\n",
    "    escribir_lista_negra(lista_negra, ruta)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "retirar_tweets_usuarios <- function(dataframe, usuarios) {\n",
    "    return (\n",
    "        dataframe %>%\n",
    "            filter(!User %in% usuarios)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets: `ahorcarme_complete.csv` y `colgarme_complete.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos los datos del archivo 'ahorcarme_complete.csv'\n",
    "publicaciones_ahorcarme <- obtener_dataframe('ahorcarme_complete.csv', datos)\n",
    "publicaciones_colgarme <- obtener_dataframe('colgarme_complete.csv', datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Filtramos los usuarios que tengan más de 5 publicaciones\n",
    "publicaciones_ahorcarme_reducidas <- obtener_publicaciones(publicaciones_ahorcarme, 5)\n",
    "publicaciones_colgarme_reducidas <- obtener_publicaciones(publicaciones_colgarme, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos la lista de los usuarios que aparecen más de 5 veces en las publicaciones\n",
    "\n",
    "usuarios_ahorcarme <- publicaciones_ahorcarme_reducidas$User\n",
    "usuarios_colgarme <- publicaciones_colgarme_reducidas$User\n",
    "\n",
    "lista_negra <- juntar_usuarios(\n",
    "    c(\n",
    "        usuarios_ahorcarme,\n",
    "        usuarios_colgarme\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(lista_negra)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, podemos revisar manualmente los tweets de los usuarios que colocamos en la lista negra bajo este criterio, para determinar si realmente sus publicaciones son despreciables para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos los tweets de estos usuarios, para revisarlos manualmente\n",
    "\n",
    "# Ahorcarme\n",
    "tweets_lisneg_ahorcarme <- publicaciones_ahorcarme_reducidas %>%\n",
    "    filter(User %in% lista_negra)\n",
    "\n",
    "# Colgarme\n",
    "tweets_lisneg_colgarme <- publicaciones_colgarme_reducidas %>%\n",
    "    filter(User %in% lista_negra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos los tweets en un archivo\n",
    "\n",
    "# Ahorcarme\n",
    "escribir_dataframe(tweets_lisneg_ahorcarme, \"./datos_limpios/datos_por_revisar/tweets_lisneg_ahorcarme.csv\")\n",
    "# Colgarme\n",
    "escribir_dataframe(tweets_lisneg_colgarme, \"./datos_limpios/datos_por_revisar/tweets_lisneg_colgarme.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de revisar los tweets, encontré un fragmento que hace alusión a una connotación sexual de la palabra concurrente. Por lo tanto, aquellos usuarios cuyas publicaciones que contienen la palabra \"ahorcarme\" serán añadidos a la lista negra.\n",
    "\n",
    "Es importante considerar que, si bien emplean la palabra bajo un concepto sexual, puede no ser la única forma en la que la usan; de hecho, logré encontrar varios perfiles en los que hacen un uso sexual y violento del verbo. De tal manera, de este grupo de usuarios que tergiversan de esta forma la acción, únicamente se seleccionarán aquellos que emplean la palabra exclusivamente en un contexto sexual, es decir, en todos los tweets que les fueron recolectados que les pertenecen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fragmento\n",
    "frags_frase = c(\n",
    "    \"ahorcame por dios ahorcame hasta el punto de que tus manos\"\n",
    ")\n",
    "\n",
    "# Localizar los tweets que contengan la frase, y de ellos tomar los usuarios, para agregarlos a los que ya\n",
    "# estaban listados, siempre y cuando todos los tweets del usuario contengan la frase\n",
    "\n",
    "usuarios_frags_ahorcarme <- obtenerUsuariosRepetitivos(publicaciones_ahorcarme, frags_frase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro son fragmentos de una canción, en los tweets con la palabra clave \"colgarme\"\n",
    "\n",
    "Los fragmentos son los siguientes:\n",
    "\n",
    "<blockquote>\n",
    "    <ul>\n",
    "        <li> Niña, dame una pestaña de tus ojos para colgarme de amor por ti </li>\n",
    "        <li> Colgarme de cualquiera que le gusta trasnochar </li>\n",
    "    </ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fragmentos\n",
    "frags_frase = c(\n",
    "    \"Niña, dame una pestaña de tus ojos para colgarme de amor por ti\",\n",
    "    \"Colgarme de cualquiera que le gusta trasnochar\"\n",
    ")\n",
    "# Localizar los tweets que contengan la frase, y de ellos tomar los usuarios, para agregarlos a los que ya\n",
    "# estaban listados.\n",
    "\n",
    "usuarios_frags_colgarme <- obtenerUsuariosRepetitivos(publicaciones_colgarme, frags_frase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igualmente, se removerá cualquier uso de la llamada en el contexto de una conversación telefónica.\n",
    "\n",
    "Por su parte, otros usos de la palabra que se refieran a una acción como \"depender de\" o \"aprovecharse de\" podrían requerir de otro tipo de servicios para ser procesados.\n",
    "\n",
    "En [este notebook](./filtrado_publicaciones_por_contexto.ipynb) estaré trabajando en otros usos de la palabra a través de los servicios que ofrece Microsoft Azure. Indagaré en el uso de los servicios cognitivos o las integraciones con OpenAI para proveer un filtro de las publicaciones con un procesamiento del lenguaje más intuitivo y preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# No quitamos al usuario completamente porque el uso de la palabra \"colgar\" en una llamada no le excluye de usarla con\n",
    "# intenciones autolesivas, por lo que nos limitamos a excluir esas publicaciones\n",
    "\n",
    "palabras_contexto_telefonico <- c(\n",
    "    \"llamar\",\n",
    "    \"marcar\",\n",
    "    \"responder\",\n",
    "    \"contestar\"\n",
    ")\n",
    "\n",
    "# Demostrando el uso del paquete tm\n",
    "raices_palabras_contexto_telefonico <- wordStem(palabras_contexto_telefonico, language = \"spanish\")\n",
    "cat(raices_palabras_contexto_telefonico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminamos los tweets que contengan alguna conjugación de los verbos que indican que se habla de una llamada\n",
    "\n",
    "tweets_colgarme_limpios <- excluir_tweets_segun_verbos(\n",
    "    publicaciones_colgarme,\n",
    "    palabras_contexto_telefonico,\n",
    "    \"spanish\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Escribir tweets para no depender de la sesión de R\n",
    "escribir_dataframe(tweets_colgarme_limpios, \"./datos_limpios/datos_por_revisar/tweets_colgarme_limpios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Escribir los mismos tweets, pero extrayendo aquellos cuyos usuarios aparecen con frecuencia\n",
    "\n",
    "escribir_dataframe(\n",
    "    obtener_publicaciones(tweets_colgarme_limpios, 5),\n",
    "    \"./datos_limpios/datos_por_revisar/tweets_colgarme_limpios_5_ocurrencias.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregar los usuarios a la lista de usuarios\n",
    "\n",
    "lista_negra <- juntar_usuarios(\n",
    "    c(\n",
    "        lista_negra,\n",
    "        usuarios_frags_ahorcarme,\n",
    "        usuarios_frags_colgarme\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Escribimos las listas de usuarios para no depender de la sesión de R\n",
    "writeLines(usuarios_frags_ahorcarme, \"./datos_limpios/datos_por_revisar/usuarios_frag_ahorcarme.txt\")\n",
    "writeLines(usuarios_frags_colgarme, \"./datos_limpios/datos_por_revisar/usuarios_frag_colgarme.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Escribir la lista negra en un archivo\n",
    "writeLines(\n",
    "    lista_negra,\n",
    "    file = \"./datos_limpios/datos_por_revisar/lista_negra.txt\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de aquí, una vez que hallamos revisado la lista negra de usuarios, vamos a limpiar los conjuntos de datos para obtener únicamente la información útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Excluir a estos usuarios de los datasets, pero a partir de la lista negra de usuarios ya revisada\n",
    "\n",
    "# Leer lista\n",
    "lista_negra_revisada <- readLines(\"./datos_limpios/lista_negra_revisada.txt\", encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Reescribir lista negra, para obtener sólo valores únicos\n",
    "lista_negra_revisada <- unique(lista_negra_revisada) %>% sort()\n",
    "writeLines(lista_negra_revisada, \"./datos_limpios/lista_negra_revisada.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ahorcarme\n",
    "publicaciones_ahorcarme <- retirar_tweets_usuarios(publicaciones_ahorcarme, lista_negra_revisada)\n",
    "\n",
    "# Colgarme\n",
    "publicaciones_colgarme <- retirar_tweets_usuarios(tweets_colgarme_limpios, lista_negra_revisada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos los tweets en un archivo\n",
    "\n",
    "# Ahorcarme\n",
    "escribir_dataframe(publicaciones_ahorcarme, \"./datos_limpios/ahorcarme_filtered.csv\")\n",
    "\n",
    "# Colgarme\n",
    "escribir_dataframe(publicaciones_colgarme, \"./datos_limpios/colgarme_filtered.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset `necesito_ayuda_complete.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset encontramos las siguientes stopwords:\n",
    "\n",
    "- Estoy jugando #VenezuelaQuiz y necesito ayuda con esta imagen ¿La reconoces?\n",
    "- ¡Necesito ayuda con mis árboles!\n",
    "- Entra aquí\n",
    "- Entra en\n",
    "- _Usuarios con “1D” en su nombre (fans de One Direction solicitando ayuda para conseguir boletos)_\n",
    "- One Direction\n",
    "- sorteo\n",
    "- concurso\n",
    "- Sigan a\n",
    "- Síganme\n",
    "- rt\n",
    "- retweet\n",
    "- MG\n",
    "- MG AL COMENTARIO\n",
    "- bonus\n",
    "\n",
    "Las usaremos para obtener a usuarios que utilicen esas frases en sus publicaciones, y los añadiremos a la lista negra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Se lee el dataset\n",
    "\n",
    "datos_necesito_ayuda <- read.csv(file = \"./datos/twitter/necesito_ayuda_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_necesito_ayuda <- c(\n",
    "    \"Estoy jugando #VenezuelaQuiz y necesito ayuda con esta imagen ¿La reconoces?\",\n",
    "    \"¡Necesito ayuda con mis árboles!\",\n",
    "    \"Entra aquí\",\n",
    "    \"Entra en\",\n",
    "    \"1D\",\n",
    "    \"One Direction\",\n",
    "    \"sorteo\",\n",
    "    \"concurso\",\n",
    "    \"Sigan a\",\n",
    "    \"Siganme\",\n",
    "    \"Síganme\",\n",
    "    \"rt\",\n",
    "    \"retweet\",\n",
    "    \"retuit\",\n",
    "    \"MG\",\n",
    "    \"MG AL COMENTARIO\",\n",
    "    \"bonus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_negra_necesito_ayuda <- obtener_usuarios_segun_palabras(\n",
    "    dataframe = datos_necesito_ayuda,\n",
    "    palabras = stopwords_necesito_ayuda\n",
    ")\n",
    "\n",
    "datos_necesito_ayuda_limpios <- retirar_tweets_usuarios(\n",
    "    datos_necesito_ayuda,\n",
    "    usuarios_ls_negra_necesito_ayuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregamos los usuarios a la lista negra\n",
    "\n",
    "anexar_a_lista(usuarios_ls_negra_necesito_ayuda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, retiraremos tweets que contengan enlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "datos_necesito_ayuda_limpios <- retirar_tweets_con_enlaces(datos_necesito_ayuda_limpios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "escribir_dataframe(\n",
    "    datos_necesito_ayuda_limpios,\n",
    "    \"./datos_limpios/twitter/necesito_ayuda_cleaned.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos, además, propaganda política en los tweets de los usuarios. A menudo, etiquetan al (al año de 2023) presidente de Venezuela, Nicolás Maduro, en sus publicaciones.\n",
    "\n",
    "No obstante, si revisamos estas cuentas podremos percatarnos de que en sus publicaciones existen peticiones por recursos, pues son personas en situación de calle, desempleo, o que no tienen acceso a servicios básicos. Por ello, se tomarán estos usuarios y se colocaran en una \"lista gris\", para que puedan ser examinados después por especialistas y determinar si su situación les puede suscitar conductas depresivas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset `perdon_por_todo_complete.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset encontramos estas frases que podrían ser de interés excluir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "expresiones_perdon_por_todo <- c(\n",
    "    \"Perdón por ser fiel, entregar todo de mí, quererte como a nadie\",\n",
    "    \"Gracias por enseñarme lo que debo mejorar\",\n",
    "    \"al querer ser perfecto se equivoca todo el mundo\",\n",
    "    \"Ya no queda valor para mirarnos de nuevo y pedirnos\",\n",
    "    \"Gracias por todo y perdón por ser tan básico\",\n",
    "    \"Corazón, perdón por todo el daño\"\n",
    ")\n",
    "\n",
    "datos_perdon_por_todo <- read.csv(file = \"./datos/twitter/perdon_por_todo_complete_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_negra_perdon <- obtenerUsuariosRepetitivos(\n",
    "    dataset = datos_perdon_por_todo,\n",
    "    textosClave = expresiones_perdon_por_todo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_negra_perdon[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregamos los usuarios a la lista negra\n",
    "\n",
    "anexar_a_lista(usuarios_ls_negra_perdon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Retiramos los usuarios del dataset y los guardamos en un archivo\n",
    "datos_perdon_por_todo_limpios <- retirar_tweets_usuarios(\n",
    "    datos_perdon_por_todo,\n",
    "    usuarios_ls_negra_perdon\n",
    ")\n",
    "\n",
    "# Retiramos los tweets con URLs\n",
    "\n",
    "datos_perdon_por_todo_limpios <- retirar_tweets_con_enlaces(\n",
    "    datos_perdon_por_todo_limpios\n",
    ")\n",
    "\n",
    "escribir_dataframe(\n",
    "    datos_perdon_por_todo_limpios,\n",
    "    \"./datos_limpios/twitter/perdon_por_todo_cleaned.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset `suicida_complete.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Se leen los datos\n",
    "\n",
    "datos_suicida <- read.csv(file = \"./datos/twitter/suicida_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# De los usuarios que tienen la palabra “suicida” en el nombre, se crea un dataset aparte y estos se quitan del dataset,\n",
    "# pues no necesariamente llevan la palabra en el tweet\n",
    "\n",
    "usuarios_nombre_llevan_suicida <- datos_suicida %>%\n",
    "    filter(grepl(\"suici\", User, ignore.case = TRUE)) %>%\n",
    "    distinct(User) %>%\n",
    "    pull(User)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Se guardan los datos de estos usuarios en un archivo\n",
    "\n",
    "datos_usuarios_llevan_suicida <- datos_suicida %>%\n",
    "    filter(User %in% usuarios_nombre_llevan_suicida)\n",
    "\n",
    "escribir_dataframe(\n",
    "    datos_usuarios_llevan_suicida,\n",
    "    \"./datos_limpios/twitter/usuarios_nombre_llevan_suicida_complete.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por su parte, debido a que tenemos como objetivos sitios de noticias también y estos están siendo reportados en un archivo adicional, vamos a reconocerlos y registrarlos en la lista negra y este archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "patrones_noticieros <- \n",
    "c(\n",
    "    \"noticia\",\n",
    "    \"period\",\n",
    "    \"noti\",\n",
    "    \"news\",\n",
    "    \"nws\",\n",
    "    \"www\",\n",
    "    \"deport\",\n",
    "    \"juegos\",\n",
    "    \"diario\",\n",
    "    \"futbol\",\n",
    "    \"canal\",\n",
    "    \"cronica\",\n",
    "    \"espectaculo\",\n",
    "    \"radio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_noticieros <- datos_suicida %>%\n",
    "    filter(grepl(paste(patrones_noticieros, collapse = \"|\"), User, ignore.case = TRUE)) %>%\n",
    "    distinct(User) %>%\n",
    "    pull(User) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Usuarios noticieros: 4740\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'Noticias_Ec'</li><li>'UNoticias'</li><li>'NoticiasTribuna'</li><li>'NoticiasRCN'</li><li>'SuNoticiero'</li><li>'noticiasLGTB'</li><li>'PUTOnews'</li><li>'AmaneceNews'</li><li>'orpnoticias'</li><li>'PeriodicoZocalo'</li><li>'Hera_Noticias'</li><li>'ControlNoticias'</li><li>'NotiEspartano'</li><li>'NotiSeguridadMX'</li><li>'AztecaNoticias'</li><li>'QroNotiLocales'</li><li>'Venezolanonews'</li><li>'seguridad_news'</li><li>'24noticiareport'</li><li>'Noti_Momento'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Noticias\\_Ec'\n",
       "\\item 'UNoticias'\n",
       "\\item 'NoticiasTribuna'\n",
       "\\item 'NoticiasRCN'\n",
       "\\item 'SuNoticiero'\n",
       "\\item 'noticiasLGTB'\n",
       "\\item 'PUTOnews'\n",
       "\\item 'AmaneceNews'\n",
       "\\item 'orpnoticias'\n",
       "\\item 'PeriodicoZocalo'\n",
       "\\item 'Hera\\_Noticias'\n",
       "\\item 'ControlNoticias'\n",
       "\\item 'NotiEspartano'\n",
       "\\item 'NotiSeguridadMX'\n",
       "\\item 'AztecaNoticias'\n",
       "\\item 'QroNotiLocales'\n",
       "\\item 'Venezolanonews'\n",
       "\\item 'seguridad\\_news'\n",
       "\\item '24noticiareport'\n",
       "\\item 'Noti\\_Momento'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Noticias_Ec'\n",
       "2. 'UNoticias'\n",
       "3. 'NoticiasTribuna'\n",
       "4. 'NoticiasRCN'\n",
       "5. 'SuNoticiero'\n",
       "6. 'noticiasLGTB'\n",
       "7. 'PUTOnews'\n",
       "8. 'AmaneceNews'\n",
       "9. 'orpnoticias'\n",
       "10. 'PeriodicoZocalo'\n",
       "11. 'Hera_Noticias'\n",
       "12. 'ControlNoticias'\n",
       "13. 'NotiEspartano'\n",
       "14. 'NotiSeguridadMX'\n",
       "15. 'AztecaNoticias'\n",
       "16. 'QroNotiLocales'\n",
       "17. 'Venezolanonews'\n",
       "18. 'seguridad_news'\n",
       "19. '24noticiareport'\n",
       "20. 'Noti_Momento'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Noticias_Ec\"     \"UNoticias\"       \"NoticiasTribuna\" \"NoticiasRCN\"    \n",
       " [5] \"SuNoticiero\"     \"noticiasLGTB\"    \"PUTOnews\"        \"AmaneceNews\"    \n",
       " [9] \"orpnoticias\"     \"PeriodicoZocalo\" \"Hera_Noticias\"   \"ControlNoticias\"\n",
       "[13] \"NotiEspartano\"   \"NotiSeguridadMX\" \"AztecaNoticias\"  \"QroNotiLocales\" \n",
       "[17] \"Venezolanonews\"  \"seguridad_news\"  \"24noticiareport\" \"Noti_Momento\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(paste0(\"Usuarios noticieros: \", length(usuarios_noticieros)))\n",
    "usuarios_noticieros[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregamos los usuarios a la lista negra\n",
    "\n",
    "anexar_a_lista(usuarios_noticieros)\n",
    "\n",
    "# Agregamos además los usuarios al archivo de noticieros\n",
    "anexar_a_lista(\n",
    "    usuarios_noticieros,\n",
    "    ruta = \"./datos_limpios/lista_pags_noticias.txt\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, comenzamos a elaborar la lista negra para este conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Se marcan en la lista negra usuarios con publicaciones que contengan las siguientes palabras/expresiones/fragmentos:\n",
    "expresiones_suicida <-\n",
    "c(\n",
    "    \"terrorista\",\n",
    "    \"ataque\",\n",
    "    \"atacante\",\n",
    "    \"piloto\",\n",
    "    \"aerol\", # Por aerolínea\n",
    "    \"avión\",\n",
    "    \"avion\",\n",
    "    \"bomba\",\n",
    "    \"atentado\",\n",
    "    \"masacre\",\n",
    "    \"RT\",\n",
    "    \"noticias\",\n",
    "    \"noticia\",\n",
    "    \"noti\",\n",
    "    \"news\",\n",
    "    \"presunto\",\n",
    "    \"polic\", # Por policía\n",
    "    \"ONU\",\n",
    "    \"OTAN\",\n",
    "    \"iran\",\n",
    "    \"irán\",\n",
    "    \"pakist\" # Por Pakistán\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las expresiones \"se suicida\" y \"escuadrón\" también son usadas por algunas personas con su cuenta personal.\n",
    "\n",
    "No obstante, sus publicaciones típicamente no superan las 10. Esto se tomará en cuenta, para colocar en la lista negra a los usuarios siempre y cuando rebasen este límite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Lista negra base\n",
    "usuarios_ls_negra_suicida_base <- obtener_usuarios_segun_palabras(\n",
    "    dataframe = datos_suicida,\n",
    "    palabras = expresiones_suicida\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Se toman estos usuarios y se revisa que sus publicaciones rebasen las 10. Estos irán a la lista negra\n",
    "\n",
    "exp_suicida_especiales <- c(\n",
    "    \"escuadr\",\n",
    "    \"se suicida\"\n",
    ")\n",
    "\n",
    "usuarios_ls_negra_suicida_escuad_sesuic <- obtener_publicaciones(\n",
    "        dataframe = datos_suicida,\n",
    "        pub_minimas = 10\n",
    "    ) %>%\n",
    "    filter(grepl(paste(exp_suicida_especiales, collapse = \"|\"), Content, ignore.case = TRUE)) %>%\n",
    "    distinct(User) %>%\n",
    "    pull(User)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'foroson123'</li><li>'murillocasanova'</li><li>'cb24tv'</li><li>'PuebladeHoy'</li><li>'laguaiqueri1'</li><li>'MauroECast'</li><li>'CUMANAINFORMA'</li><li>'Guillermodelao'</li><li>'nacotemx'</li><li>'TresCanicas'</li><li>'Manuelglezp'</li><li>'Noticias_Ec'</li><li>'patyvargaas'</li><li>'lavozdeturmero'</li><li>'algonzalez72'</li><li>'juanlol0'</li><li>'marce_godoy1'</li><li>'gudnieta'</li><li>'azorindorian'</li><li>'comuvm_rdd'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'foroson123'\n",
       "\\item 'murillocasanova'\n",
       "\\item 'cb24tv'\n",
       "\\item 'PuebladeHoy'\n",
       "\\item 'laguaiqueri1'\n",
       "\\item 'MauroECast'\n",
       "\\item 'CUMANAINFORMA'\n",
       "\\item 'Guillermodelao'\n",
       "\\item 'nacotemx'\n",
       "\\item 'TresCanicas'\n",
       "\\item 'Manuelglezp'\n",
       "\\item 'Noticias\\_Ec'\n",
       "\\item 'patyvargaas'\n",
       "\\item 'lavozdeturmero'\n",
       "\\item 'algonzalez72'\n",
       "\\item 'juanlol0'\n",
       "\\item 'marce\\_godoy1'\n",
       "\\item 'gudnieta'\n",
       "\\item 'azorindorian'\n",
       "\\item 'comuvm\\_rdd'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'foroson123'\n",
       "2. 'murillocasanova'\n",
       "3. 'cb24tv'\n",
       "4. 'PuebladeHoy'\n",
       "5. 'laguaiqueri1'\n",
       "6. 'MauroECast'\n",
       "7. 'CUMANAINFORMA'\n",
       "8. 'Guillermodelao'\n",
       "9. 'nacotemx'\n",
       "10. 'TresCanicas'\n",
       "11. 'Manuelglezp'\n",
       "12. 'Noticias_Ec'\n",
       "13. 'patyvargaas'\n",
       "14. 'lavozdeturmero'\n",
       "15. 'algonzalez72'\n",
       "16. 'juanlol0'\n",
       "17. 'marce_godoy1'\n",
       "18. 'gudnieta'\n",
       "19. 'azorindorian'\n",
       "20. 'comuvm_rdd'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"foroson123\"      \"murillocasanova\" \"cb24tv\"          \"PuebladeHoy\"    \n",
       " [5] \"laguaiqueri1\"    \"MauroECast\"      \"CUMANAINFORMA\"   \"Guillermodelao\" \n",
       " [9] \"nacotemx\"        \"TresCanicas\"     \"Manuelglezp\"     \"Noticias_Ec\"    \n",
       "[13] \"patyvargaas\"     \"lavozdeturmero\"  \"algonzalez72\"    \"juanlol0\"       \n",
       "[17] \"marce_godoy1\"    \"gudnieta\"        \"azorindorian\"    \"comuvm_rdd\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "usuarios_ls_negra_suicida <- juntar_usuarios(\n",
    "    c(\n",
    "        usuarios_ls_negra_suicida_base,\n",
    "        usuarios_ls_negra_suicida_escuad_sesuic\n",
    "    )\n",
    ")\n",
    "\n",
    "usuarios_ls_negra_suicida[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Agregamos los usuarios a la lista negra\n",
    "anexar_a_lista(usuarios_ls_negra_suicida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Retiramos los usuarios del dataset\n",
    "\n",
    "datos_suicida_limpios <- retirar_tweets_usuarios(\n",
    "    datos_suicida,\n",
    "    usuarios_ls_negra_suicida\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Retiramos los tweets con URLs también\n",
    "\n",
    "datos_suicida_limpios <- datos_suicida_limpios %>%\n",
    "    retirar_tweets_con_enlaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    }
   ],
   "source": [
    "# Escribimos los datos limpios\n",
    "\n",
    "escribir_dataframe(\n",
    "    datos_suicida_limpios,\n",
    "    \"./datos_limpios/twitter/suicida_cleaned.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets `queria_estar_muerto_cleaned.csv` y `queria_morir_cleaned.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estos conjuntos de datos únicamente retiraremos los enlaces. Por otro lado, el filtrado de los usuarios se hizo manualmente para el conjunto de datos `queria_estar_muerto`, mientras que para el otro se definirá una lista negra y sí se retirarán los usuarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    }
   ],
   "source": [
    "datos_queria_estar_muerto_limpios <- read.csv(file = \"./datos_limpios/twitter/queria_estar_muerto_cleaned.csv\") %>%\n",
    "    retirar_tweets_con_enlaces()\n",
    "\n",
    "escribir_dataframe(\n",
    "    datos_queria_estar_muerto_limpios,\n",
    "    \"./datos_limpios/twitter/queria_estar_muerto_cleaned.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_negra_queria_morir <- c(\n",
    "    \"_LaPista_\",\n",
    "    \"_Showzzy\",\n",
    "    \"24HDeportesTVN\",\n",
    "    \"A_bran_es\",\n",
    "    \"abc_familia\",\n",
    "    \"AcentoVNoticias\",\n",
    "    \"ActualidadRT\",\n",
    "    \"alexascarpita\",\n",
    "    \"alikton\",\n",
    "    \"AnaRMarti\",\n",
    "    \"AndresCardeenas\",\n",
    "    \"aylinj1D\",\n",
    "    \"bekiaes\",\n",
    "    \"BogotaET\",\n",
    "    \"brayan_insur_12\",\n",
    "    \"cactus24noticia\",\n",
    "    \"canaltn8\",\n",
    "    \"capdevielleja\",\n",
    "    \"CaraotaDigital\",\n",
    "    \"CCNesnoticias\",\n",
    "    \"Citytv\",\n",
    "    \"Con_Sentimiento\",\n",
    "    \"ConservadorMX\",\n",
    "    \"contrapuntovzla\",\n",
    "    \"CorreodelCaroni\",\n",
    "    \"CorreodeOaxaca\",\n",
    "    \"CronicaUno\",\n",
    "    \"Cuida_Emocional\",\n",
    "    \"CulturaColectiv\",\n",
    "    \"Danirobonita\",\n",
    "    \"demamasdepapas\",\n",
    "    \"DirectionerB1D\",\n",
    "    \"DolarToday\",\n",
    "    \"DoralHoy\",\n",
    "    \"e_consulta\",\n",
    "    \"Ecoteuve\",\n",
    "    \"EfectoNaim\",\n",
    "    \"El_Universal_Mx\",\n",
    "    \"el_pais\",\n",
    "    \"elandivar\",\n",
    "    \"elespectador\",\n",
    "    \"elias0998\",\n",
    "    \"eljuntaletrass\",\n",
    "    \"elpaismexico\",\n",
    "    \"ElPitazoTV\",\n",
    "    \"eltelevisero\",\n",
    "    \"ELTIEMPO\",\n",
    "    \"EncarnaSanchezJ\",\n",
    "    \"enlazadot\",\n",
    "    \"EP_Mundo\",\n",
    "    \"EresCurioso\",\n",
    "    \"escuelalemat\",\n",
    "    \"EvaRiquelme3\",\n",
    "    \"exitoina\",\n",
    "    \"fabiomb\",\n",
    "    \"Franz_Strada\",\n",
    "    \"FrenteFantasma\",\n",
    "    \"Gabyg1233\",\n",
    "    \"gpascual96\",\n",
    "    \"GuzmanJoseVicen\",\n",
    "    \"HayQueSaberlo\",\n",
    "    \"HipcritasNoWD\",\n",
    "    \"historiasqlaten\",\n",
    "    \"HiwatarixValkov\",\n",
    "    \"hoyextremadura\",\n",
    "    \"HoyTeInformo\",\n",
    "    \"infobae\",\n",
    "    \"infobaeamerica\",\n",
    "    \"InfoVelozCom\",\n",
    "    \"ivan_ramibal\",\n",
    "    \"JosiriOsorio\",\n",
    "    \"juaneinacio\",\n",
    "    \"la_patilla\",\n",
    "    \"LaCronicaDeHoy\",\n",
    "    \"LAFAYETTERICO\",\n",
    "    \"lasillarota\",\n",
    "    \"Leandro68455164\",\n",
    "    \"legs3535\",\n",
    "    \"los_replicantes\",\n",
    "    \"lucasecastillo\",\n",
    "    \"m1espectaculos\",\n",
    "    \"MicaelaRomero_1\",\n",
    "    \"MiLugarFavorit2\",\n",
    "    \"NicaNws\",\n",
    "    \"NicoKontreras\",\n",
    "    \"NubeyChloe\",\n",
    "    \"Pajaropolitico\",\n",
    "    \"pame_hunter\",\n",
    "    \"PaparazziRevis\",\n",
    "    \"paulinotech\",\n",
    "    \"pauliqw591\",\n",
    "    \"PeterKrasno\",\n",
    "    \"publimetro_dep\",\n",
    "    \"PublimetroChile\",\n",
    "    \"punto7osorno\",\n",
    "    \"Punto7ptomontt\",\n",
    "    \"punto7temuco\",\n",
    "    \"QuePasaVenado\",\n",
    "    \"RadioImpacTotal\",\n",
    "    \"RedRadioVe\",\n",
    "    \"Reportajes_org\",\n",
    "    \"RunRunesWeb\",\n",
    "    \"sabiastuque_\",\n",
    "    \"SadujDark\",\n",
    "    \"safal4776033\",\n",
    "    \"segurosybanca\",\n",
    "    \"SinEmbargoMX\",\n",
    "    \"SoyModaNet\",\n",
    "    \"superconfirmado\",\n",
    "    \"SwaggieVane\",\n",
    "    \"tar88t\",\n",
    "    \"TeleSaltillo\",\n",
    "    \"teleSURtv\",\n",
    "    \"thecliniccl\",\n",
    "    \"TVNotasmx\",\n",
    "    \"un_dato\",\n",
    "    \"valeriecortesnh\",\n",
    "    \"VenadoTuerto140\",\n",
    "    \"VenezolanoPres_\",\n",
    "    \"VestidoFiesta\",\n",
    "    \"yakibracamontes\"\n",
    ")\n",
    "\n",
    "# Ag\"regamos los usuarios a la lista negra\",\n",
    "anexar_a_lista(usuarios_ls_negra_queria_morir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "datos_queria_morir <- read.csv(file = \"./datos/twitter/queria_morir_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_noticieros_queria_morir <- datos_queria_morir %>%\n",
    "    filter(grepl(paste(patrones_noticieros, collapse = \"|\"), User, ignore.case = TRUE)) %>%\n",
    "    distinct(User) %>%\n",
    "    pull(User)\n",
    "\n",
    "# Lista negra base\n",
    "anexar_a_lista(usuarios_noticieros_queria_morir)\n",
    "# Agregamos además los usuarios al archivo de noticieros\n",
    "anexar_a_lista(\n",
    "    usuarios_noticieros_queria_morir,\n",
    "    ruta = \"./datos_limpios/lista_pags_noticias.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_negra_queria_morir_join <- juntar_usuarios(\n",
    "\n",
    "    c(\n",
    "        usuarios_ls_negra_queria_morir,\n",
    "        usuarios_noticieros_queria_morir,\n",
    "        obtenerUsuariosRepetitivos(\n",
    "            dataset = datos_queria_morir,\n",
    "            textosClave = c(\n",
    "                \"Tu mirada fue el espejo , donde quería ver nuestro reflejo\",\n",
    "                \"de amor, para que supieras cómo y cuánto te quería\",\n",
    "                \"matarme a besos, yo quería morir en tus labios\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "datos_queria_morir_limpios <- datos_queria_morir %>%\n",
    "    retirar_tweets_usuarios(usuarios_ls_negra_queria_morir_join) %>%\n",
    "    retirar_tweets_con_enlaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    }
   ],
   "source": [
    "escribir_dataframe(\n",
    "    datos_queria_morir,\n",
    "    ruta_archivo = './datos_limpios/twitter/queria_morir_cleaned.csv'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### datasets `quiero_estar_muerto_complete.csv` y `quisiera_estar_muerto.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "datos_quiero_estar_muerto <- read.csv(file = \"./datos/twitter/quiero_estar_muerto_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_quiero_estar_muerto <- c(\n",
    "    \"carolinad_bot\",\n",
    "    \"botpoemas\",\n",
    "    \"rockolaperu\",\n",
    "    \"Samyurin\",\n",
    "    \"wxzm\"\n",
    ")\n",
    "\n",
    "usuarios_frags_quiero_estar_muerto <- obtenerUsuariosRepetitivos(\n",
    "    dataset = datos_quiero_estar_muerto,\n",
    "    textosClave = c(\n",
    "        \"dile que estoy muerto, dile que estoy seco\",\n",
    "        \"dile que estoy seco que quiero estar con ella\",\n",
    "        \"quiero soñar con los ojos abiertos, quiero sentir, sentir el fuego\",\n",
    "        \"me tienes muerto, sin rumbo como un vagabundo\",\n",
    "        \"y siempre quiero estar muerto, para seguir con mi boca enredada en tus cabellos\",\n",
    "        \"muerto de la risa, así es como quiero estar\",\n",
    "        \"Yo te quiero con el alma, que el alma nunca muere\"\n",
    "    ),\n",
    "    ignorarSignosPunt = TRUE # El dataset es chico y podemos permitirnos la búsqueda de repetición\n",
    "                             # con frases con o sin signos de puntuación\n",
    ")\n",
    "\n",
    "datos_quiero_estar_muerto_limpios <- datos_quiero_estar_muerto %>%\n",
    "    retirar_tweets_usuarios(usuarios_ls_quiero_estar_muerto) %>%\n",
    "    retirar_tweets_con_enlaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'carolinad_bot'</li><li>'botpoemas'</li><li>'rockolaperu'</li><li>'Samyurin'</li><li>'wxzm'</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'carolinad\\_bot'\n",
       "\\item 'botpoemas'\n",
       "\\item 'rockolaperu'\n",
       "\\item 'Samyurin'\n",
       "\\item 'wxzm'\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'carolinad_bot'\n",
       "2. 'botpoemas'\n",
       "3. 'rockolaperu'\n",
       "4. 'Samyurin'\n",
       "5. 'wxzm'\n",
       "6. NA\n",
       "7. NA\n",
       "8. NA\n",
       "9. NA\n",
       "10. NA\n",
       "11. NA\n",
       "12. NA\n",
       "13. NA\n",
       "14. NA\n",
       "15. NA\n",
       "16. NA\n",
       "17. NA\n",
       "18. NA\n",
       "19. NA\n",
       "20. NA\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"carolinad_bot\" \"botpoemas\"     \"rockolaperu\"   \"Samyurin\"     \n",
       " [5] \"wxzm\"          NA              NA              NA             \n",
       " [9] NA              NA              NA              NA             \n",
       "[13] NA              NA              NA              NA             \n",
       "[17] NA              NA              NA              NA             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "usuarios_ls_quiero_estar_muerto[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "anexar_a_lista(usuarios_ls_quiero_estar_muerto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    }
   ],
   "source": [
    "escribir_dataframe(\n",
    "    datos_quiero_estar_muerto_limpios,\n",
    "    ruta_archivo = './datos_limpios/twitter/quiero_estar_muerto_cleaned.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "datos_quisiera_estar_muerto <- read.csv(file = \"./datos/twitter/quisiera_estar_muerto_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "usuarios_ls_quisiera_estar_muerto <- c(\n",
    "    \"escribircancion\",\n",
    "    \"nirumi80\"\n",
    ")\n",
    "\n",
    "usuarios_frags_qusiera_estar_muerto <- obtenerUsuariosRepetitivos(\n",
    "    dataset = datos_quisiera_estar_muerto,\n",
    "    textosClave = c(\n",
    "        \"no quisiera estar muerto, al menos no tanto\"\n",
    "    ),\n",
    "    ignorarSignosPunt = TRUE # El dataset es chico y podemos permitirnos la búsqueda de repetición\n",
    "                             # con frases con o sin signos de puntuación\n",
    ")\n",
    "\n",
    "datos_quisiera_estar_muerto_limpios <- datos_quisiera_estar_muerto %>%\n",
    "    retirar_tweets_usuarios(usuarios_ls_quisiera_estar_muerto) %>%\n",
    "    retirar_tweets_con_enlaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'escribircancion'</li><li>'nirumi80'</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li><li>NA</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'escribircancion'\n",
       "\\item 'nirumi80'\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\item NA\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'escribircancion'\n",
       "2. 'nirumi80'\n",
       "3. NA\n",
       "4. NA\n",
       "5. NA\n",
       "6. NA\n",
       "7. NA\n",
       "8. NA\n",
       "9. NA\n",
       "10. NA\n",
       "11. NA\n",
       "12. NA\n",
       "13. NA\n",
       "14. NA\n",
       "15. NA\n",
       "16. NA\n",
       "17. NA\n",
       "18. NA\n",
       "19. NA\n",
       "20. NA\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"escribircancion\" \"nirumi80\"        NA                NA               \n",
       " [5] NA                NA                NA                NA               \n",
       " [9] NA                NA                NA                NA               \n",
       "[13] NA                NA                NA                NA               \n",
       "[17] NA                NA                NA                NA               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "usuarios_ls_quisiera_estar_muerto[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "anexar_a_lista(usuarios_ls_quisiera_estar_muerto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    }
   ],
   "source": [
    "escribir_dataframe(\n",
    "    datos_quisiera_estar_muerto_limpios,\n",
    "    ruta_archivo = './datos_limpios/twitter/quisiera_estar_muerto_cleaned.csv'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset `matarme_complete.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, podemos también ordenar la lista negra, para que cuando esta se requiera, se pueda acceder a sus valores más eficientemente a través de un algoritmo de búsqueda binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lista_negra_final <- cargar_lista_negra() %>% sort()\n",
    "\n",
    "escribir_lista_negra(\n",
    "    lista_negra_final#,\n",
    "    # retirar_duplicados = TRUE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ordenamos la lista de páginas de noticias\n",
    "usuarios_noticieros <- readLines(\"./datos_limpios/lista_pags_noticias.txt\") %>%\n",
    "    # unique() %>%\n",
    "    sort()\n",
    "\n",
    "# Se guardan los datos de estos usuarios en un archivo\n",
    "writeLines(usuarios_noticieros, \"./datos_limpios/lista_pags_noticias.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset `suicidal_corpus_complete_translated.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conjunto de datos incluyen textos clasificados como aquellos que denotan riesgo suicida y aquellos que no.\n",
    "\n",
    "Para que el corpus de datos sea útil, el director del proyecto se solicitó que cada texto tenga, cuanto menos, 5 palabras, y que no se repitan textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dataset_corpus_suicida <- read.csv(file = \"./datos/otros/suicidal_corpus_complete_translated.csv\")\n",
    "\n",
    "filas_prev <- nrow(dataset_corpus_suicida)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afortunadamente, también se cuenta con el dataset antes de realizar la traducción de los textos que se van a procesar. Por lo tanto, debido a que este dataset será sometido a una revisión manual de las traducciones por parte del equipo, vamos a tomar el texto original y traducido para que las correcciones sean más precisas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Las columnas de clasificación son iguales:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_corpus_suicida_ingles <- read.csv(file = \"./datos/otros/suicidal_corpus_complete_original.csv\")\n",
    "\n",
    "# Nos aseguramos de que las columnas que contienen la clasificación de los textos sean iguales, para que, al momento\n",
    "# de unir los datasets, no corrompamos la información\n",
    "\n",
    "print(\"Las columnas de clasificación son iguales:\")\n",
    "identical(\n",
    "    dataset_corpus_suicida$cls,\n",
    "    dataset_corpus_suicida_ingles$cls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dataset_corpus_suicida <- data.frame(\n",
    "    text_original = dataset_corpus_suicida_ingles$text,\n",
    "    text_translated = dataset_corpus_suicida$text,\n",
    "    cls = dataset_corpus_suicida$cls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in write.csv(dataframe, file = ruta_archivo, row.names = FALSE, :\n",
      "\"attempt to set 'col.names' ignored\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Se redujeron 470 filas\"\n"
     ]
    }
   ],
   "source": [
    "dataset_corpus_suicida_reducido <- dataset_corpus_suicida %>%\n",
    "    filter(str_count(text_translated, \"\\\\w+\") >= 5) %>%\n",
    "    distinct(text_translated, .keep_all = TRUE)\n",
    "\n",
    "escribir_dataframe(\n",
    "    dataset_corpus_suicida_reducido,\n",
    "    ruta_archivo = './datos_limpios/datos_por_revisar/otros/suicidal_corpus_reduced_joined.csv'\n",
    ")\n",
    "\n",
    "filas_post <- nrow(dataset_corpus_suicida_reducido)\n",
    "\n",
    "filas_reducidas <- filas_prev - filas_post\n",
    "\n",
    "print(paste0(\"Se redujeron \", filas_reducidas, \" filas\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
